# -*- coding: utf-8 -*-
"""ML su Emissioni Lombardia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qQi5oq2ZVodUlz18-prjCqIINGaRvmaH

# ANALISI delle EMISSIONI (Lombardia 2014)

# FASE 1: analisi, pulizia e data visualization
"""

!pip install tensorflow
!pip install transformers accelerate torchvision --quiet
!pip install --upgrade scikit-learn joblib threadpoolctl
!pip install xlrd

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import torch
import os
import pickle
import tensorflow as tf

from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error
from sklearn.metrics import silhouette_score
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA

from IPython.display import display
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from PIL import Image
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

"""Stampiamo a schermo il dataframe per visualizzarlo."""

DF = pd.read_excel("/content/Emissioni.xls");
X = DF.copy()
X.head()

"""## **COMPRENSIONE DELLE FEATURES**
* Settore = categoria del veicolo

* Combustibile = tipo di carburante utilizzato

* Tipo legislativo = normativa ambientale di riferimento

* Periodo = intervallo temporale in cui è stata applicata la normativa

* Consumo specifico = consumo medio di carburante

Per comprendere meglio i dati, segue una spiegazione delle sostanze chimiche analizzate del dataset.

*   SO₂ (Anidride solforosa) = gas tossico prodotto dalla combustione di combustibili fossili contenenti zolfo

*   NOₓ (Ossidi di azoto) = gruppo di gas reattivi (NO e NO₂) che contribuiscono allo smog e alla formazione di ozono troposferico

*   COV (Composti Organici Volatili) = molecole organiche che evaporano facilmente

*   CH₄ (Metano) = potente gas serra

*   CO (Monossido di carbonio) = gas incolore e tossico prodotto da combustione incompleta

*   CO₂ (Anidride carbonica) = principale gas serra emesso da attività umane

*   N₂O (Protossido di azoto) = gas serra con effetto molto più potente della CO₂

*   NH₃ (Ammoniaca) = gas pungente emesso da attività agricole

*   PM2.5 = particelle sospese con diametro ≤ 2.5 µm

*   PM10 = particelle sospese con diametro ≤ 10 µm

*   PTS (Polveri Totali Sospese) = insieme di tutte le particelle solide e liquide sospese nell’aria

Stampiamo le informazioni del dataframe.
"""

X.info()

X.tail()

"""Ci siamo resi conto che il dataset presentava delle righe non utilizzabili durante l'analisi, quindi abbiamo proceduto ad eliminarle e ad uniformare eventuali colonne per unità misura."""

# copiamo i dati sotto forma di lista in un nuovo df e eliminiamo colonne nulle o sbagliate
new_columns = X.iloc[1].tolist()
new_columns = [col if pd.notna(col) else f'Unnamed_{i}' for i, col in enumerate(new_columns)]
X.columns = new_columns
df = X.iloc[3:].copy()
df = df.iloc[:60].copy()
df = df.reset_index(drop=True)

# cambio dell'unità di misura del consumo di combustibile e CO2 (da g/cm3 a mg/cm3)
df['Consumo specifico'] = df['Consumo specifico'] * 1000
df['CO2'] = df['CO2'] * 1000

df.head()

df.tail()

df.info()

"""Convertiamo i dati numerici di tipo "Object" in double, in modo a poterli utilizzare nella maniera appropriata successivamente."""

# colonne da convertire
columnsF = ['Consumo specifico', 'SO2', 'NOx', 'COV', 'CH4', 'CO', 'CO2', 'N2O', 'NH3', 'PM2.5', 'PM10', 'PTS']

# ciclo per convertirle
for col in columnsF:
    df[col] = pd.to_numeric(df[col], errors='coerce')

df.info()

"""Utilizziamo describe per vedere un po' di dati del df."""

df.describe()

"""Tutti gli inquinanti e il consumo specifico hanno 60 osservazioni, che non risultano essere abbastanza per fornire un risultato preciso ed accurato; possono comunque fornire una idea generale.

Le **medie** variano molto tra le sostanze: ad esempio, SO₂ ha una media molto elevata rispetto ad altri inquinanti, indicando una forte variabilità tra le fonti.

Le **deviazioni standard** sono spesso alte, segno che le emissioni cambiano sensibilmente in base al tipo di veicolo, carburante e normativa.

Le **emissioni di CO₂** sono presenti in tutti i veicoli e mostrano valori consistenti, confermando il suo ruolo come **indicatore principale** di impatto ambientale.

Contiamo quanti valori nulli sono presenti per ogni colonnna.
"""

df.isnull().sum()

"""Notata la presenza di un valore null nella colonna 'Periodo', abbiamo stampato la relativa riga."""

null = df[df.isnull().any(axis = 1)]
null

"""Una volta individuato l'indice della riga corrispondente, abbiamo proceduto ad eliminarla."""

df = df.drop(index = 42)
df = df.reset_index(drop=True)
df.isnull().sum()

"""## ANALISI DELLE FEATURES

Abbiamo stampato i boxplot e gli istogrammi delle features numeriche.
"""

# Boxplot delle features
colEscluse = ['Settore', 'Combustibile', 'Tipo legislativo', 'Periodo']

for i, col in enumerate(df.columns):
  if col not in colEscluse:
        fig, axs = plt.subplots(1, 2, figsize=(8, 4))
        axs[0].boxplot(df[col])
        axs[0].set_title(f"Boxplot {col}")
        axs[1].hist(df[col])
        axs[1].set_title(f"Istogramma {col}")
        plt.tight_layout()
        plt.show()

"""Data la presenza di veicoli di varie dimensioni e pesi risulta comunque normale la presenza di consumi anche fuori dalla media, quindi non li considerermo dati **anomali**; li analizzeremo comunque per comprendere meglio la variabilità del dataset.

Data la grande differenza tra i settori, abbiamo deciso di separare le categorie di veicoli ed eseguire le analisi separatamente per ognuno, per poi andare a confrontare i risultati successivamente.

"""

# dividiamo il df
df_automobili = df[df['Settore'] == 'Automobili']
df_leggeri = df[df['Settore'] == 'Veicoli leggeri < 3.5 t']
df_pesanti = df[df['Settore'] == 'Veicoli pesanti > 3.5 t e autobus']
df_ciclomotori = df[df['Settore'] == 'Ciclomotori (< 50 cm3)']
df_motocicli = df[df['Settore'] == 'Motocicli (> 50 cm3)']

df_automobili.describe()

df_leggeri.describe()

"""Per una ricerca più approfondita e consapevole dei dati anomali abbiamo stampato separatamente, per ogni features numerica, tutti gli istogrammi dei settori."""

# scegliamo i dati
colonna_da_confrontare = 'Consumo specifico'
settori_unici = df['Settore'].unique()
n_bins = 20

# definiamo il minimo e massimo
x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05
y_max_global = 0

# cicla sui settori e crea l'istogramma
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

# estetica del grafico
y_max_padded = y_max_global * 1.10
n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

# crea i grafici
for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]
    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))
    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)
    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)

# inserisce nella griglia i diversi istogrammi per il confronto
for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

# stampa dei grafici
plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

"""Come ci aspettavamo, dal grafico si può notare come i veicoli più leggeri consumino di meno. Ergo, il peso e il consumo sono correlati.

Adesso rieseguiamo lo stesso codice per tutte le features.
"""

colonna_da_confrontare = 'SO2'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

"""I veicoli che non produzono So2 sono tutti a metano e metano/GPL, questo dimostra che non sono outliers."""

df[df['SO2'] == 0]

colonna_da_confrontare = 'NOx'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

colonna_da_confrontare = 'COV'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

colonna_da_confrontare = 'CH4'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

"""Notiamo un outliers, decidiamo quindi di verificare la realisticità dello stesso: essendo un veicolo pesante euro II, possiamo considerare normale questo tipo di consumi."""

df[df['CH4'] > 4000]

colonna_da_confrontare = 'CO'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

"""Sono due veicoli di tipo legislativo "Conventional", quindi i dati rientrano nel consumo medio (secondo diverse fotni su internet)."""

df[df['CO'] > 20000]

colonna_da_confrontare = 'CO2'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

"""Il metano produce più CO2, soprattutto nei veicoli pesanti."""

df[df['CO2'] > 1400000]

colonna_da_confrontare = 'N2O'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

"""È plausibile che i veicoli diesel più datati (classificati come "Conventional") abbiano valori di N₂O pari a zero nei dataset , per due motivi:
1. **Assenza di misurazioni storiche:** in passato, le emissioni di N₂O non erano sempre misurate o regolamentate
2. **Tecnologia del motore:** i motori diesel più vecchi potrebbero effettivamente produrre quantità trascurabili di N₂O rispetto ad altri inquinanti
"""

df[df['N2O'] == 0]

colonna_da_confrontare = 'NH3'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

df[df['NH3'] > 130]

"""In questo caso, abbiamo tutti veicoli Euro II.      ervwewevwevqwevwe


"""

colonna_da_confrontare = 'PM2.5'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

colonna_da_confrontare = 'PM10'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

colonna_da_confrontare = 'PTS'
settori_unici = df['Settore'].unique()
n_bins = 20

x_min = df[colonna_da_confrontare].min()
x_max = df[colonna_da_confrontare].max()
x_range = x_max - x_min
x_min_padded = x_min - x_range * 0.05
x_max_padded = x_max + x_range * 0.05

y_max_global = 0
for settore in settori_unici:
    df_settore = df[df['Settore'] == settore]
    counts, bins = np.histogram(df_settore[colonna_da_confrontare], bins=n_bins, range=(x_min, x_max))
    current_y_max = counts.max()
    if current_y_max > y_max_global:
        y_max_global = current_y_max

y_max_padded = y_max_global * 1.10

n_cols = 3
n_rows = (len(settori_unici) + n_cols - 1) // n_cols

fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()

for i, settore in enumerate(settori_unici):
    df_settore = df[df['Settore'] == settore]

    axes[i].hist(df_settore[colonna_da_confrontare], bins=n_bins, edgecolor='black', color='skyblue',
                 range=(x_min, x_max))

    axes[i].set_xlim(x_min_padded, x_max_padded)
    axes[i].set_ylim(0, y_max_padded)


    axes[i].set_title(settore)
    axes[i].set_xlabel(colonna_da_confrontare)
    axes[i].set_ylabel('Frequenza')
    axes[i].grid(True, linestyle='--', alpha=0.7)


for j in range(len(settori_unici), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle(f'Distribuzione del {colonna_da_confrontare} per Settore', y=1.02, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

"""Veicoli diversi con tipi legislativi e combustibili diversi possono produrre più o meno inquinanti, quindi non è possibile definire degli outliers specifici; e dato che il dataset è già ridotto, preferiamo mantere una ampia variabilità dei dati.

## CORRELAZIONE

Abbiamo proceduto a cercare le correlazioni tra le features nei vari tipi di settori, attraverso l'uso di heatmap.
"""

# Calcolo la correlazione tra le features (solo numeriche) nelle automobili
corr = df_automobili.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

# Calcolo la correlazione tra le features (solo numeriche) nei veicoli leggeri
corr = df_leggeri.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

# Calcolo la correlazione tra le features (solo numeriche) nei veicoli pesanti
corr = df_pesanti.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

# Calcolo la correlazione tra le features (solo numeriche) nei ciclomotori
corr = df_ciclomotori.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

"""Da questo grafico possiamo notare che le correlazioni di NOx con le altre sostanze siano completamente, o quasi totalmente, inverse (-1.00). A livello chimico, ciò potrebbe derivare dalle reazioni presenti nei motori dei veicoli, in quanto NOx, reagendo, si consuma e aumenta la formazione degli altri agenti inquinanti: questo spiega perché al suo diminuire le altre sostanze invece aumentino."""

# Calcolo la correlazione tra le features (solo numeriche) nei motocicli
corr = df_motocicli.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

"""Come evidenziato sopra, all'interno dei motori avvengono reazioni chimiche che possono portare gli inquinanati a reagire tra loro.

Elementi come NH3 e N2O appartengano ai reagenti per la produzione di sostanze come i particolati (PM2.5, PM10 e PTS).

Abbiamo notato che indipendentemente dal tipo di veicolo ci sono forti correlazioni tra la produzione di CO2 e il consumo specifico e tra gli inquinanti PM2.5, PM10, PTS.
Questo indica che questo tipi di inquinanti non sono specifici per settore a differenza di altri come SO2 per i veicoli leggeri e il COV per i veicoli pesanti.

Abbiamo deciso anche di controllare le varie correlazioni tra le features e i diversi tipi di carburante. Per farlo, abbiamo suddiviso come prima il dataframe in base al combustibile e abbiamo creato le heatmap.
"""

# dividiamo il df
df_benzinaverde = df[df['Combustibile'] == 'benzina verde']
df_diesel = df[df['Combustibile'] == 'diesel']
df_metanoGPL = df[df['Combustibile'] == 'metano/GPL']
df_metano = df[df['Combustibile'] == 'metano']

df_benzinaverde.head()

# Calcolo la correlazione tra le features (solo numeriche) nei veicoli a benzina
corr = df_benzinaverde.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

# Calcolo la correlazione tra le features (solo numeriche) nei veicoli a diesel
corr = df_diesel.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

# Calcolo la correlazione tra le features (solo numeriche) nei veicoli a metalo e GPL
corr = df_metanoGPL.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

# Calcolo la correlazione tra le features (solo numeriche) nei veicoli a metalo e GPL
corr = df_metano.select_dtypes(include='number').corr()

# Visualizzo la matrice di correlazione
fig, ax = plt.subplots(1, figsize=(9, 7))
sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap='bwr', ax=ax)
plt.show()

"""Dalle heatmap relative ai carburanti possiamo notare con avvengano varie reazioni chimiche relative al combustibili.
Abbiamo osservato forti correlazioni per i veicoli a diesel e metano, perchè sono i tipi di combustibili che inquinano maggiormente l'ambiente, producendo tutti gli inquinanti elencati.
Inoltre, si nota come metano e metano/GPL non presentino le correlazioni di SO2, questo perchè questi tipi di di carburante non producono questo inquinante.
"""

df[df['SO2'] == 0]

"""# FASE 2
Prepara il set di dati da usare per applicare almeno 2 algoritmi di ML di classificazione. Ovvero l’obiettivo è capire in base al consumo di combustibile come possono essere raggruppati tra loro i settori. La classificazione dei settori risultante è in linea con il tipo di combustibile? Ci sono combustibili assimilabili per consumo? Ci sono differenze tra i risultati degli algoritmi scelti?

Nei passagi precedenti, abbiamo constatato che il "periodo" e il "tipo legislativo" risultano ridondanti, in quanto ci sono delle corrispondenze tra i valori delle due categorie. Per verificarlo, stampiamo una tabella di contingenza: questa mostra quali e quante sono le corrispondenze tra i valori delle due colonne.
"""

# Crea tabella di contingenza
pivot = df.groupby(["Periodo", "Tipo legislativo"]).size().unstack(fill_value=0)
plt.figure(figsize=(12, 8))
sns.heatmap(pivot, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("Corrispondenza tra Periodo e Tipo legislativo")
plt.ylabel("Periodo")
plt.xlabel("Tipo legislativo")
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""L'analisi mostra che, nella quasi totalità dei casi, esiste una corrispondenza 1:1 tra Periodo e Tipo Legislativo (un quadrato colorato per colonna). Abbiamo identificato alcune eccezioni (valori che presentano 2 quadrati colorati) che possono essere considerate come anomalie, data la minoranza numerica.
Dato ciò, abbiamo deciso di eliminare dal dataframe la colonna del periodo in quanto il tipo legislativo è di maggiore comprensione ed evita rindonzanze.
"""

df.drop('Periodo', axis=1, inplace=True)
df_vecchio = df

"""Procediamo a convertire le colonne di tipo *qualitativo* in *quantitativo*, tramite l'utilizzo di etichette, per semplificare il successivo processo di classificazione."""

le_consumo = LabelEncoder()
le_settore = LabelEncoder()
le_combustibile = LabelEncoder()
le_tipo_legislativo = LabelEncoder()

df['Settore'] = le_settore.fit_transform(df['Settore'])
df['Combustibile'] = le_combustibile.fit_transform(df['Combustibile'])
df['Tipo legislativo'] = le_tipo_legislativo.fit_transform(df['Tipo legislativo'])
df.head()

df_dummies = df
df_dummies.head()

"""Settore:

*   Automobili: 0
*   Ciclomotori (< 50 cm3) : 1
*   Motocicli (> 50 cm3) : 2
*   Veicoli leggeri < 3.5 t: 3
*   Veicoli pesanti > 3.5 t e autobus: 4

---

Combustibili:

*   Benzina Verde: 0
*   Diesel: 1
*   Metano: 2
*   Metano/GPL: 3

---

Tipi legislativi:

*   Conventional: 0
*   EEV: 1
*   Euro 0: 2
*   Euro 1 - 91/441/EEC: 3
*   Euro 1 - 93/59/EEC: 4
*   Euro 2 - 94/12/EC: 5
*   Euro 2 - 96/69/EC: 6
*   Euro 3 - 98/69/EC Stage 2000: 7
*   Euro 4 - 98/69/EC Stage 2005: 8
*   Euro 5: 9
*   Euro 5 - EC 715/2007: 10
*   Euro 6: 11
*   Euro 6 - EC 715/2007: 12
*   Euro I - 91/542/EEC Stage I: 13
*   Euro I - 97/24/EC: 14
*   Euro I - 97/24/EC Stage I: 15
*   Euro II: 16
*   Euro II - 91/542/EEC Stage II: 17
*   Euro II - 97/24/EC Stage II: 18
*   Euro III: 19
*   Euro III - 1999/96/EC: 20
*   Euro III - 1999/96/EC step 1: 21
*   Euro IV - COM(1998) 776: 22
*   Euro V - COM(1998) 776: 23
*   Euro VI - Reg EC 595/2009: 24

## CLUSTERING

Abbiamo deciso di utilizzare il clustering perchè non necessita di un target specifico, e ci permette di capire in quali settori (non noti) possono essere suddivisi i dati.
"""

# Scegliamo le features
features = df[['Consumo specifico', 'Tipo legislativo', 'Combustibile', 'Settore']]

# Standardizzazione
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Metodo del gomito per scegliere il numero di cluster
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    inertia.append(kmeans.inertia_)

# stampa elbow plot
plt.figure(figsize=(8, 5))
sns.lineplot(x=range(1, 11), y=inertia, marker='o')
plt.xlabel('Numero di cluster')
plt.ylabel('Inertia')
plt.title('Elbow Plot')
plt.show()

"""Grazie a questo elbow plot riusciamo a intuire che il numero ideale di cluster da utilizzare è 7, poichè dopo quel punto notiamo una diminuzione della pendenza della curva, ciò indica che precedentemente e succesivamente al punto 7 ci sia un peggioramento nella descrizione della variabilità dei dati.

Applichiamo due diversi modelli di clustering: prima il K-means, dove dobbiamo indicare noi in quanti cluster suddividere i dati, e successivamente un clustering agglomerativo, che non richiede un input di numero di cluster.

###K-means
"""

# Applica K-Means
kmeans = KMeans(n_clusters=7, random_state=42)
df['cluster'] = kmeans.fit_predict(scaled_features)

# Visualizza i cluster
sns.boxplot(x='cluster', y='Consumo specifico', data=df)
plt.title('Distribuzione del consumo per cluster')
plt.show()

"""Calcoliamo la Silohuette score per valutare la qualità di un clustering, ovvero quanto un'osservazione è simile al proprio cluster rispetto agli altri.
Inoltre utilizziamo la PCA per ridurre la dimensione dei dati, catturando la maggior parte delle informazioni.
"""

# Calcolo del Silhouette Score
score = silhouette_score(scaled_features, df['cluster'])
print(f"Silhouette Score: {score:.3f}")

# Visualizzazione con PCA
pca = PCA(n_components=2)
components = pca.fit_transform(scaled_features)

df['pca1'] = components[:, 0]
df['pca2'] = components[:, 1]

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='pca1', y='pca2', hue='cluster', palette='Set2')
plt.title('Visualizzazione dei cluster con PCA')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster')
plt.tight_layout()
plt.show()

"""**INTERPRETAZIONE DELLA SILOHUETTE SCORE:**
* maggiore 0.7: clustering molto buono
* 0.5 - 0.7: buono
* 0.3 - 0.5: accettabile
* < 0.3: debole, forse troppi o troppo pochi cluster


**CONTROLLO PCA**

Separazione tra i gruppi:
* Se i cluster sono ben separati nel grafico, significa che K-Means ha trovato gruppi distinti.
* Se i cluster si sovrappongono molto, potrebbe esserci rumore o i gruppi non sono ben definiti.

Forma e densità:
* Cluster compatti e ben definiti sono un buon segno.
* Cluster sparsi o allungati potrebbero indicare che i dati non sono ben rappresentati da K-Means.

Distribuzione dei punti:
* Se un cluster è molto più grande o più piccolo degli altri, potrebbe essere utile rivedere il numero di cluster o le feature usate.

Dato lo score ottenuto (0.514), possiamo considerare il nostro modello di cluster buono, e dal grafico possiamo inoltre constatare che i punti all'interno del proprio cluster sono vicini, ma comunque distinti dai punti dei cluster adiacenti.

###Clustering Agglomerativo
"""

# Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=3)
df['agglo_cluster'] = agglo.fit_predict(scaled_features)

# Silhouette Score
score = silhouette_score(scaled_features, df['agglo_cluster'])
print(f"Silhouette Score: {score:.3f}")

# PCA per visualizzazione
pca = PCA(n_components=2)
components = pca.fit_transform(scaled_features)
df['pca1'] = components[:, 0]
df['pca2'] = components[:, 1]

# Visualizzazione
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='pca1', y='pca2', hue='agglo_cluster', palette='Set1')
plt.title('Agglomerative Clustering')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend(title='Cluster')
plt.tight_layout()
plt.show()

"""Dato lo score ottenuto (0.395), notiamo che questo modello di cluster ha una qualità peggiore. Ciò secondo noi è dovuto dalla netta diminuizione di numero di cluster utilizzati dall'algoritmo (da 7 a 3 cluster usati).

Secondo la PCA i punti risultano comunque ben distinti ma meno densi per ogni cluster.

##Tabelle di contingenza e analisi dei combustibili

Eseguiamo nuovamente un controllo delle corrispondenze tra il tipo di combustibile e i cluster, per verificare se i cluster ricalcano i combustibili.
"""

# Crea tabella di contingenza
pivot = df.groupby(["Combustibile", "cluster"]).size().unstack(fill_value=0)
plt.figure(figsize=(8, 5))
sns.heatmap(pivot, annot=True, fmt='d', cmap='Reds', cbar=False)
plt.title("Corrispondenza tra Combustibile e Cluster")
plt.ylabel("Combustibile")
plt.xlabel("Cluster")
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""La maggioranza dei cluster presenta una corrispondenza con un certo tipo di combustibile, tranne i cluster 0 e 2 che sono distribuiti equamente tra benzina e diesel.

Eseguiamo lo stesso controllo sui cluster generati automaticamente dal clustering agglomerativo.
"""

# Crea tabella di contingenza
pivot = df.groupby(["Combustibile", "agglo_cluster"]).size().unstack(fill_value=0)
plt.figure(figsize=(8, 5))
sns.heatmap(pivot, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.title("Corrispondenza tra Combustibile e Cluster")
plt.ylabel("Combustibile")
plt.xlabel("Cluster")
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""Risulta meno preciso in quanto abbiamo più combustibili all'interno dello stesso cluster, a conferma di quanto evidenziato dal silouhette score.

Abbiamo assegnato a delle variabili la media dei consumi specifici per tipo di combustibile per controllare se ce ne fossero di assimilabili.
"""

benzina = df[df["Combustibile"] == 0]["Consumo specifico"].mean()
diesel = df[df["Combustibile"] == 1]["Consumo specifico"].mean()
metano = df[df["Combustibile"] == 2]["Consumo specifico"].mean()
GPL = df[df["Combustibile"] == 3]["Consumo specifico"].mean()

print("media benzina: ", benzina)
print("media diesel: ", diesel)
print("media metano: ", metano)
print("media metano/gpl: ", GPL)

average_consumption = df.groupby('Combustibile')['Consumo specifico'].mean().reset_index()

plt.figure(figsize=(10, 6))
sns.barplot(x='Combustibile', y='Consumo specifico', data=average_consumption, palette='viridis', hue='Combustibile', legend=False)
plt.title('Consumo medio per Combustibile')
plt.xlabel('Combustibile')
plt.ylabel('Consumo medio')
plt.xticks(rotation=0)
plt.show()

"""Possiamo notare che benzina (0) e metano/GPL (3) sono assimilabili per consumi.

Il metano (2) si contraddistingue per essere poco performante (alto consumo).

---

# FASE 3

Occorre prevedere l’andamento del PM10. Qual è l’agente che maggiormente incide sul PM10? Applica sui dati il modello di rete neurale scegliendo la funzione di attivazione e il numero di layer.

Per questa fase, abbiamo deciso di riprendere il dataframe originale, per utilizzare le variabili qualitative.
"""

df = df_vecchio

"""Iniziamo con la ricerca delle correlazioni di PM10 con le altre features."""

# Calcolo della matrice di correlazione per tutte le colonne numeriche
correlation_matrix = df_dummies.corr()

# Estraiamo le correlazioni con PM10
pm10_correlations = correlation_matrix['PM10'].sort_values(ascending=False)

print("Correlazioni con PM10")
print(pm10_correlations)

# Heatmap delle correlazioni
plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Matrice di Correlazione delle Features')
plt.show()

"""Dalla matrice di correlazione possiamo vedere che i valori più collegati a quelli di PM10 sono:
*   PM 2.5
*   PTS

Non ci sono altre correlazioni forti.

##PREVISIONE DELL'ANDAMENTO DI PM10

Per prevedere l'andamento di PM10, utilizzeremo un modello di rete feedforewarding neurale tramite l'uso di KERAS.

Dividiamo i dati in train e test set per addestrare la rete neurale
"""

# Copiamo il df per sicurezza
df_copia = df

# Creiano X e Y
X = df_copia.drop(columns=['PM10'])
y = df_copia['PM10']

# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 48)

# Scaliamo le features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""Creiamo una rete neurale con 2 layer nascosti  (non danno un output visibile al'utente) e che lavorano sui dati di training:
 * layer da 64 neuroni
 * layer da 32 neuroni

Successivamente c'è un layer non nascosto per fornire l'output in modo lineare (valore continuo).

L'attivazione avviene tramite RELU.

Utilizziamo ADAM per l'ottimizzazione.
"""

# rete neurale feedforewarding con 2 layer nascpsti con attivazione Relu e output lineare
model = Sequential()
# primo layer: riceve dati (64 neuroni)
model.add(Dense(64, input_dim= X_train.shape[1], activation='relu'))
# secondo layer: elabora (32 neuroni)
model.add(Dense(32, activation='relu'))
# terzo layer: restituisice l'output (1 neurone)
model.add(Dense(1, activation= 'linear'))

# ottimizzazione con Adam
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()

"""Addestramento del modello.

Epoch indica un passaggio completo del dataset di training attraverso gli algoritmi di ML.
"""

history = model.fit(X_train_scaled, y_train, epochs=200, batch_size=32, validation_split=0.2, verbose=1)

"""Calcoliamo MAE (differenza tra valore reale e valore previsto) e R^2 (legame tra la variabilità dei dati e la correttezza del modello statistico utilizzato) per vedere quanto il modello è stato addestrato correttamente sui dati di training."""

y_pred = model.predict(X_test_scaled).flatten()

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae: .3f}")
print(f"R2: {r2: .3f}")

plt.figure()
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.legend()
plt.title("Curva di apprendimento")
plt.tight_layout()
plt.savefig("curva_apprendimento.png")
plt.show()
plt.close()

"""Il grafico rappresenta la curva di apprendimento, ovvero l’andamento dell’errore della rete neurale durante l’addestramento.
* Train Loss (blu): l’MSE calcolato sui dati di addestramento.
* Val Loss (arancione): l’errore calcolato sui dati di validazione (dati che il modello non vede durante il training, usati per testarne la generalizzazione).

Dato che le due curve convergono lentamente e non si sovrappongono mai effettivamente, possiamo dedurre che l'apprendimento del modello è lento e non del tutto ottimale.

Salviamo il modello creato in locale per usarlo in futuro.
"""

model.save("PM10_model.keras")
pd.to_pickle(scaler, "PM10_scaler.pkl")

"""Test: proviamo il modello di rete neurale appena creato sul dataset originale senza i valori del PM10 per valutare la correttezza dei valori calcolati."""

df_prova = df.drop(columns=['PM10'])

scaler = pickle.load(open("PM10_scaler.pkl", "rb"))
model = tf.keras.models.load_model("PM10_model.keras")

X_new_scaled = scaler.transform(df_prova)
pred_pm10 = model.predict(X_new_scaled).flatten()

for i, val in enumerate(pred_pm10):
  print(f"Stima PM10: {val: .2f}, valore reale {df['PM10'][i]:.2f} microg/cm3")

"""Calcoliamo alcuni coefficienti di errore per capire l'affidabilità del modello."""

# Root Mean Squared Error (RMSE)
mse = mean_squared_error(df['PM10'], pred_pm10)
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE): {rmse:.2f} microg/cm3")

# R-squared (Coefficiente di Determinazione)
r2 = r2_score(df['PM10'], pred_pm10)
print(f"R-squared (R2): {r2:.4f}")

if r2 > 0.7:
    print("\nIl modello spiega una buona parte della varianza nei dati reali.")
elif r2 > 0.4:
    print("\nIl modello spiega una moderata parte della varianza nei dati reali.")
else:
    print("\nIl modello non spiega bene la varianza nei dati reali.")

print(f'Errore medio sulle previsioni:  {rmse:.2f} microg/cm3')

"""Proviamo a creare una nuova rete neurale con 3 layer nascosti per vedere se i risultati migliorano."""

# rete neurale feedforewarding con 3 layer nascosti con attivazione Relu e output linear
model = Sequential()
# primo layer: riceve dati (64 neuroni)
model.add(Dense(64, input_dim= X_train.shape[1], activation='relu'))
# secondo layer: elabora (32 neuroni)
model.add(Dense(32, activation='relu'))
# terzo layer: elabora (16 neuroni)
model.add(Dense(16, activation='relu'))
# quarto layer: restituisice l'output (1 neurone)
model.add(Dense(1, activation= 'linear'))

# ottimizzazione con Adam
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()

history = model.fit(X_train_scaled, y_train, epochs=200, batch_size=32, validation_split=0.2, verbose=1)

"""Effettiamo i controlli precedenti."""

y_pred = model.predict(X_test_scaled).flatten()
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae: .3f}")
print(f"R-squared (R2) Score: {r2: .3f}")

plt.figure()
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.legend()
plt.title("Curva di apprendimento")
plt.tight_layout()
plt.savefig("curva_apprendimento.png")
plt.show()
plt.close()

"""Come ci aspettavamo, un modello di rete neurale con più layers performa meglio data la maggiore quantità di operazioni eseguite sui dati.
Man mano che le epoch aumentano, le due curve scendono rapidamente, segno che la rete sta imparando a prevedere il PM10.
Entrambe le curve convergono verso valori bassi, quindi il modello sta generalizzando bene.
"""

model.save("PM10_model.keras")
pd.to_pickle(scaler, "PM10_scaler.pkl")

# togliamo la colonna per verificare la corretta previsione
df_prova = df.drop(columns=['PM10'])

scaler = pickle.load(open("PM10_scaler.pkl", "rb"))
model = tf.keras.models.load_model("PM10_model.keras")

X_new_scaled = scaler.transform(df_prova)
pred_pm10_2 = model.predict(X_new_scaled).flatten()

for i, val in enumerate(pred_pm10_2):
  print(f"Stima PM10 riga {i+1}: {val: .2f} microg/cm3")

# Root Mean Squared Error (RMSE)
mse = mean_squared_error(df['PM10'], pred_pm10_2)
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE): {rmse:.2f} microg/cm3")

# R-2 (Coefficiente di Determinazione)
r2 = r2_score(df['PM10'], pred_pm10_2)
print(f"R-squared (R²): {r2:.4f}")

if r2 > 0.7:
    print("\nIl modello spiega una buona parte della varianza nei dati reali.")
elif r2 > 0.4:
    print("\nIl modello spiega una moderata parte della varianza nei dati reali.")
else:
    print("\nIl modello non spiega bene la varianza nei dati reali.")

print(f'Errore medio sulle previsioni:  {rmse:.2f} microg/cm3')

"""Abbiamo constatato, grazie ai valori di MSE e R^2, che il modello di rete neurale con più layers funziona meglio nella previsione dei dati.

# FASE 4

Scegli e utilizza altri modelli previsionale, il risultato migliora? Riesci ad usare l’ai generativa per commentare i risultati ottenuti?

##Regressione multipla

Facciamo la regressione multipla per prevedere l'andamento del PM10 in relazione ai fattori più correlati, che sono PM2.5 e PTS (come evidenziato nelle fasi precedenti).
"""

y = df['PM10'].to_numpy()
X = df[['PM2.5', 'PTS']].to_numpy()

# Splittaggio del database
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
# Regressione
reg = LinearRegression().fit(X_train, y_train)

# Calcolo previsioni R^2 e RMSE
y_test_pred = reg.predict(X_test)
r2_test = r2_score(y_test, y_test_pred)
y_train_pred = reg.predict(X_train)
r2_train = r2_score(y_train, y_train_pred)
rmse = np.sqrt(root_mean_squared_error(y_test, y_pred))

print(f'Train test R2: ', r2_train)
print(f'Test test R2: ',r2_test)
print(f'RMSE: ', rmse)

plt.figure()
plt.scatter(range(len(y_test)), y_test, label='Dati reali')
plt.scatter(range(len(y_test_pred)), y_test_pred, color='red', label='Dati predetti')
plt.ylabel('PM10')
plt.title(f'Regressione Multipla sul Test Set per i valori di PM10')
plt.legend()
plt.show()

"""Data il dataset molto piccolo e le variabili sono molte correlate, può succedere che il valore di R^2 venga predetto in maniera più precisa (molto vicino all'1).

Il fatto che i due R^2 siano molto simili mostra che il modello performa in maniera analoga sia su dati noti che su dati nuovi.

Abbiamo deciso di mostrare la forte linearità tra i dati tramite due regressioni lineari singole tra PM2.5 e PTS con PM10.
"""

#Regressione linare tra PM10 e le singole features
y = df['PM10'].to_numpy()
columns = df[['PM2.5', 'PTS']]

# Creiamo la griglia
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Cicliamo sulle colonne
for c, col_name in enumerate(columns):
    # Selezioniamo l'asse
    ax = axes[c]
    # Assegniamo le X alle features
    X = df[[col_name]].to_numpy()
    # Splitting del database
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
    # Regressione Lineare
    reg = LinearRegression().fit(X_train, y_train)

    # Plotting
    X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
    y_plot_pred = reg.predict(X_plot)
    ax.scatter(X, y, label='Dati')
    ax.plot(X_plot, y_plot_pred, color='red', label='Linea di regressione')
    ax.set_xlabel(col_name)
    ax.set_ylabel('PM10')
    ax.set_title(f'Regressione Lineare {col_name}')
    ax.legend()

# Mostriamo l'immagine
plt.tight_layout()
plt.show()

# Stampa dei residui
residui = y_test - y_test_pred
plt.figure(figsize=(10, 6))
plt.scatter(range(len(residui)), residui, color='red', label='Residui')
plt.axhline(0, color='gray', linestyle='--')
plt.title('Grafico dei Residui')
plt.legend()
plt.show()

"""Dalla stampa dei residui si può notare come i valori non seguano un pattern specifico e quindi non ci siano relazioni non catturate dal nostro modello.
La media dei residui è distribuita in modo casuale attorno allo 0, quindi la differenza fra i valori reali e quelli predetti è piuttosto bassa, ergo, le predizioni si avvicinano molto ai valori reali.

##KNN

Testiamo un secondo modello di regressione per testare ulteriormente l'andamento del PM10.
"""

y = df['PM10'].to_numpy()
X = df[['PM2.5', 'PTS']].to_numpy()

# Standardizzare le x
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Splittaggio dei dati
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1)

# KNN (con numero migliore di vicini)
neigh = KNeighborsRegressor(n_neighbors=12)
neigh.fit(X_train, y_train)
y_pred = neigh.predict(X_test)

# RMSE e R^2
rmse = np.sqrt(root_mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"R^2: {r2:.2f}")

"""Possiamo notare come l'R2 sia nettamente peggiorato rispetto al modello di regressione multipla, questo potrebbe riguardare sempre poca densità dei dati."""

# Stampa dei residui
y_test_pred = reg.predict(X_test)
residui = y_test - y_test_pred
plt.figure(figsize=(10, 6))
plt.scatter(range(len(residui)), residui, color='red', label='Residui')
plt.axhline(0, color='gray', linestyle='--')
plt.title('Grafico dei Residui')
plt.legend()
plt.show()

"""Possiamo notare come non siano presenti valori sotto allo zero, questo indica che l'errore avviene sempre per eccesso. I residui sono distribuiti comunque casualmente, anche se si può notare una differenza rispetto al primo.

##GEN AI

Utiliziamo ora l'AI generativa per commentare i risultati ottenuti precedentemente.
"""

local_image_path = "/content/download2.png"

try:
    # Controlliamo se il file esiste prima di provare ad aprirlo
    if not os.path.exists(local_image_path):
        raise FileNotFoundError(f"Il file immagine '{local_image_path}' non è stato trovato.")
    # Carichiamo l'immagine
    image = Image.open(local_image_path).convert("RGB")
    print(f"Immagine caricata: {local_image_path}")
except FileNotFoundError as e:
    print(f"Errore: {e}")
    print("Assicurati che il percorso del file sia corretto e che il file esista.")
    exit()
except Exception as e:
    print(f"Si è verificato un errore durante il caricamento dell'immagine: {e}")
    exit()

display(image)

"""Dopo aver scaricato l'immagine, utilizziamo BLIP per generare una prima caption all'immagine."""

# utilizzo di BLIP
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

# Se disponibile, sposta il modello su GPU per accelerare l'elaborazione
if torch.cuda.is_available():
    model.to("cuda")
    print("Modello BLIP spostato su GPU")

# Preprocessiamo l'immagine e prepara gli input per il modello
inputs = processor(images=image, return_tensors="pt")

# Spostiamo gli input sulla GPU se il modello è lì
if torch.cuda.is_available():
    inputs = {k: v.to("cuda") for k, v in inputs.items()}

# Generiamo la descrizione (caption) dell'immagine
out = model.generate(**inputs)
caption = processor.decode(out[0], skip_special_tokens=True)
print("Didascalia generata:", caption)

"""La didascalia è un po' confusa e potrebbe essere più precisa: per questo motivo decidiamo di testare un ulteriore modello."""

# utilizzo di BLIP2
device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b").to(device)
model.eval()

inputs = processor(images=image, text="", return_tensors="pt").to(device)

model.to(inputs["pixel_values"].device)

with torch.no_grad():
    output = model.generate(**inputs, max_new_tokens=3000, do_sample=True, temperature=0.8, repetition_penalty=1.1)

caption = processor.decode(output[0], skip_special_tokens=True)
print("Didascalia dettagliata:", caption)

"""La seconda frase risulta migliorata, modificando il livello di temperatura riusciamo ad ottenere un risultato diverso in quanto questo influenza la creatività del modello.

Abbiamo provato due versioni di BLIP in combinazione con GPT-2 senza e con *attention_mask*, la quale serve per uniformare la lunghezza delle frasi (sequenze di token), e *padding:* i modelli lavorano meglio se tutte le frasi hanno la stessa lunghezza, se una frase è più corta, si aggiungono dei token finti (padding) alla fine.
"""

# utilizzo BLIP con GPT-2 senza attention_mask e padding
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

# Creazione della caption con BLIP
inputs = processor(images=image, return_tensors="pt").to(device)
caption_ids = blip_model.generate(**inputs)
caption = processor.decode(caption_ids[0], skip_special_tokens=True)

# Espandiamo la descrizione con GPT-2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)
input_text = f"Describe a scene based on: {caption}"
inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)

outputs = gpt2_model.generate(inputs, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Descrizione BLIP:", caption)
print("\nTesto creativo generato da GPT-2:")
print(generated_text)

# utilizzo BLIP con GPT-2 con attention_mask e padding
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Creiamo la caption con BLIP
input_text = f"Describe: {caption}"
inputs = tokenizer.encode_plus(
    input_text,
    return_tensors="pt",
    padding=True,
    truncation=True
).to(device)

# Generiamo l'output passando anche l'attention_mask tramite GPT-2
outputs = gpt2_model.generate(
    inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_length=200,
    num_return_sequences=1,
    no_repeat_ngram_size=2
)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Descrizione BLIP:", caption)
print("\nTesto creativo generato da GPT-2:")
print(generated_text)

"""Notiamo che con l'utilizzo della maschera e del padding l'output risulta più coerente e sensato rispetto al input fornito.

---

# Grazie per l'attenzione!
Sofia Caroli

Anna Passini

Gabriele Ricci
"""

